name: Download Memes From Reddit

on:
  schedule:
    - cron: '*/15 * * * *'
  workflow_dispatch:
    inputs:
      disable_cache_check:
        description: 'Disable cache check (true/false)'
        required: false
        default: false
        type: boolean
      show_debug:
        description: 'Enable debug messages (true/false)'
        required: false
        default: false
        type: boolean
      post_limit:
        description: 'Number of posts to fetch from each subreddit'
        required: false
        default: '5'
        type: string

jobs:
  download-memes:
    name: 🌐 Download Memes
    runs-on: ubuntu-latest
    outputs:
      metadata: ${{ steps.collect-metadata.outputs.metadata }}
    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4

      - name: 📦 Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: 🧹 Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: meme-ids-cache-
          mode: exact
        continue-on-error: true

      - name: 🚀 Restore Meme IDs Cache
        id: restore-cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-
          restore-keys: |
            meme-ids-cache-
      - name: 🧰 Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install praw requests
      - name: 🔧 Install Rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash
      - name: 📂 Set Up Rclone Config
        run: |
          mkdir -p ~/.config/rclone
          echo "${{ secrets.PIXELDRAIN_CONF }}" > ~/.config/rclone/rclone.conf
      - name: ⏬ Download Latest Memes
        id: download-memes
        env:
          CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
          DISABLE_CACHE_CHECK: ${{ github.event.inputs.disable_cache_check }}
          SHOW_DEBUG: ${{ github.event.inputs.show_debug || true }}
          POST_LIMIT: ${{ github.event.inputs.post_limit || '5' }}
        run: |
          mkdir -p cache
          python - <<EOF
          import os
          import json
          import requests
          import praw
          client_id = os.getenv("CLIENT_ID")
          client_secret = os.getenv("CLIENT_SECRET")
          user_agent = os.getenv("USER_AGENT")
          disable_cache_check = os.getenv("DISABLE_CACHE_CHECK", "false").lower() == "true"
          show_debug = os.getenv("SHOW_DEBUG", "false").lower() == "true"
          post_limit = int(os.getenv("POST_LIMIT", "5"))
          def debug_log(msg):
              if show_debug:
                  print(f"🛠️ DEBUG: {msg}")
          debug_log("✅ Initializing Reddit client in read-only mode...")
          reddit = praw.Reddit(
              client_id=client_id,
              client_secret=client_secret,
              user_agent=user_agent,
          )
          cache_file = "cache/meme_ids.json"
          debug_log(f"🔍 Loading cached IDs from {cache_file}...")
          try:
              with open(cache_file, "r") as f:
                  cached_ids = set(json.load(f))
              debug_log(f"{len(cached_ids)} cached IDs loaded.")
          except:
              debug_log("⚠️ No valid cache found.")
              cached_ids = set()
          new_ids = []
          valid_exts = ['.png','.jpg','.jpeg','.webp','.gif']
          memes_metadata = []  # List to store meme metadata
          subreddits = ["memes", "ProgrammerHumor", "dankmemes", "DirtyMemes", "rule34", "rareinsults", "futanari" ,"HardPornGifs", "funny", "pics", "science", "todayilearned", "porn_gifs"]
          for subreddit in subreddits:
              debug_log(f"🔄 Fetching latest posts from r/{subreddit}...")
              for post in reddit.subreddit(subreddit).new(limit=post_limit):
                  if disable_cache_check or post.id not in cached_ids:
                      try:
                          head = requests.head(post.url, allow_redirects=True, timeout=5)
                          ctype = head.headers.get("Content-Type","").lower()
                          if ctype.startswith("image/") or any(post.url.lower().endswith(e) for e in valid_exts):
                              print(f"🎉 New Meme Found: {post.title} | {post.url}")
                              
                              # Download the image
                              data = requests.get(post.url, timeout=10).content
                              fname = os.path.join("cache", os.path.basename(post.url))
                              with open(fname, "wb") as f:
                                  f.write(data)
                              
                              # Collect metadata for Telegram
                              meme_data = {
                                  "title": post.title,
                                  "author": post.author.name.replace("_", "-") if post.author else "Unknown",
                                  "upvotes": post.ups,
                                  "url": post.url,
                                  "filename": os.path.basename(post.url),
                                  "subreddit": subreddit
                              }
                              memes_metadata.append(meme_data)
                              
                              new_ids.append(post.id)
                      except Exception as e:
                          print(f"⚠️ Error processing post {post.id}: {e}")
          if new_ids:
              cached_ids.update(new_ids)
              with open(cache_file, "w") as f:
                  json.dump(sorted(cached_ids), f)
              print(f"🎉 Downloaded {len(new_ids)} new memes.")
          else:
              print("✨ No New Memes Found!")
          # Save metadata to file for the next steps in the workflow
          with open("cache/memes_metadata.json", "w") as f:
              json.dump(memes_metadata, f, indent=2)
          EOF
      - name: 🗂️ Collect Metadata
        id: collect-metadata
        run: |
          # Compress to single-line JSON
          metadata=$(jq -c . cache/memes_metadata.json)
          echo "metadata=$metadata" >> $GITHUB_OUTPUT
      - name: 🌐 Upload Memes with Rclone
        run: |
          rclone copy cache Pixeldrain:"💯 Memes"
      - name: 🔧 Compute Hash of Updated meme_ids.json
        id: compute-hash
        run: |
          if [ -f cache/meme_ids.json ]; then
            FILE_HASH=$(sha256sum cache/meme_ids.json | awk '{print $1}')
          else
            FILE_HASH="empty-cache"
          fi
          echo "Computed file hash: $FILE_HASH"
          echo "hash=$FILE_HASH" >> $GITHUB_ENV
      - name: 💾 Update Meme IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-${{ env.hash }}

      - name: 📖 Show meme_ids.json After
        run: |
          if [ -f cache/meme_ids.json ]; then
            echo "📄 Contents of meme_ids.json after:"
            cat cache/meme_ids.json
          else
            echo "⚠️ meme_ids.json does not exist after the script runs."
          fi
  send-to-telegram:
    name: 📤 Send Memes to Telegram
    needs: download-memes
    runs-on: ubuntu-latest
    strategy:
      matrix:
        meme: ${{ fromJson(needs.download-memes.outputs.metadata) }}
    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4

      - name: 📤 Send Meme with Metadata
        uses: appleboy/telegram-action@master
        with:
          to: ${{ secrets.TELEGRAM_TO }}
          token: ${{ secrets.TELEGRAM_TOKEN }}
          format: 'markdown'
          message: |
            🎉 **New Meme Alert!**
            📜 *Title:* ${{ matrix.meme.title }}
            🖋️ *Author:* ${{ matrix.meme.author }}
            👍 *Upvotes:* ${{ matrix.meme.upvotes }}
            🔗 *URL:* [View Post](${{ matrix.meme.url }})
          photo: cache/${{ matrix.meme.filename }}

name: Download Memes From Reddit

on:
  schedule:
    - cron: '0 * * * *' # Runs every hour
  workflow_dispatch: # Allows manual triggering of the workflow

jobs:
  download-memes:
    name: üåê Download Memes
    runs-on: ubuntu-latest
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üì¶ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: üß∞ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade git+https://github.com/praw-dev/praw.git
          pip install requests

      - name: üîÑ Restore Cache
        id: cache-restore
        uses: actions/cache@v4
        with:
          path: cache/memes.json
          key: reddit-memes-cache

      - name: ‚è¨ Download Latest Memes
        id: download-memes
        env:
          CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
        run: |
          mkdir -p cache
          touch cache/memes.json
          python - <<EOF
          import os
          import json
          import requests
          import praw
          
          # Environment variables for credentials
          client_id = os.getenv("CLIENT_ID")
          client_secret = os.getenv("CLIENT_SECRET")
          user_agent = os.getenv("USER_AGENT")
          
          print("üîß DEBUG: Initializing Reddit client in read-only mode...")
          reddit = praw.Reddit(
              client_id=client_id,
              client_secret=client_secret,
              user_agent=user_agent,
          )
          
          # Load cached URLs
          cache_file = "cache/memes.json"
          print(f"üîß DEBUG: Loading cached URLs from {cache_file}...")
          try:
              with open(cache_file, "r") as f:
                  cached_urls = set(json.load(f))
              print(f"üîß DEBUG: {len(cached_urls)} cached URLs loaded.")
          except (FileNotFoundError, json.JSONDecodeError) as e:
              print(f"‚ö†Ô∏è DEBUG: Cache file not found or invalid. Starting with an empty cache. Error: {e}")
              cached_urls = set()
          
          # Create a directory for images
          image_dir = "cache"
          os.makedirs(image_dir, exist_ok=True)
          print(f"üîß DEBUG: Ensured image directory exists at {image_dir}.")
          
          # Fetch new posts from r/memes
          print("üîß DEBUG: Fetching latest posts from r/memes...")
          new_urls = []
          for post in reddit.subreddit("memes").new(limit=10):
              print(f"üîß DEBUG: Processing post: {post.title} | URL: {post.url}")
              
              if post.url not in cached_urls:
                  try:
                      print("üîß DEBUG: Sending HEAD request to validate content type...")
                      response = requests.head(post.url, allow_redirects=True, timeout=5)
                      content_type = response.headers.get("Content-Type", "")
                      print(f"üîß DEBUG: Content-Type for {post.url} is {content_type}.")
          
                      if content_type.startswith("image/"):
                          print(f"üéâ New Meme Found:\n  Title: {post.title}\n  Author: u/{post.author}\n  Upvotes: {post.score}\n  URL: {post.url}")
                          
                          # Save the image
                          print(f"üîß DEBUG: Sending GET request to download the image...")
                          response = requests.get(post.url, timeout=10)
                          filename = os.path.join(image_dir, os.path.basename(post.url))
                          with open(filename, "wb") as f:
                              f.write(response.content)
                          print(f"‚úÖ Image saved to {filename}.")
                          new_urls.append(post.url)
                      else:
                          print(f"‚ö†Ô∏è Skipping Non-Image URL: {post.url}")
                  except requests.RequestException as e:
                      print(f"‚ùå Failed to Process URL {post.url}. Error: {e}")
              else:
                  print(f"‚ö†Ô∏è DEBUG: URL already cached: {post.url}")
          
          # Update cache
          print(f"üîß DEBUG: Updating cache with {len(new_urls)} new URLs...")
          cached_urls.update(new_urls)
          with open(cache_file, "w") as f:
              json.dump(list(cached_urls), f)
          print("‚úÖ Cache updated.")
          
          if not new_urls:
              print("‚ú® No New Memes Found!")
          else:
              print(f"üéâ {len(new_urls)} new memes downloaded.")
          EOF

      - name: üìú List All Files
        run: |
          tree

      - name: üõ†Ô∏è Save Artifacts
        if: steps.download-memes.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: memes
          path: cache/*.png,cache/*.jpg,cache/*.jpeg

      - name: üßπ Update Cache
        if: steps.download-memes.outcome == 'success'
        uses: actions/cache@v4
        with:
          path: cache/memes.json
          key: reddit-memes-cache

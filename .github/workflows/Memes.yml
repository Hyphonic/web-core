name: ‚è¨ Download Memes

on:
  schedule:
    - cron: '*/15 * * * *'
  workflow_dispatch:
    inputs:
      disable_cache_check:
        description: 'Disable cache check (true/false)'
        required: false
        default: false
        type: boolean
      show_debug:
        description: 'Enable debug messages (true/false)'
        required: false
        default: false
        type: boolean
      post_limit:
        description: 'Number of posts to fetch from each subreddit'
        required: false
        default: '5'
        type: string

jobs:
  download-memes:
    name: üåê Download Memes
    runs-on: ubuntu-latest
    outputs:
      metadata: ${{ steps.collect-metadata.outputs.metadata }}
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üì¶ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: üßπ Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: meme-ids-cache-
          mode: exact
        continue-on-error: true

      - name: üöÄ Restore Meme IDs Cache
        id: restore-cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-
          restore-keys: |
            meme-ids-cache-

      - name: üß∞ Install Dependencies
        run: |
          echo "üêç Upgrading pip..."
          python -m pip install --upgrade pip
          echo "üì¶ Installing PRAW, Requests, and yt_dlp..."
          pip install praw requests yt-dlp

      - name: üîß Install Rclone
        run: |
          echo "üîÑ Installing Rclone..."
          curl https://rclone.org/install.sh | sudo bash

      - name: üìÇ Set Up Rclone Config
        run: |
          echo "üìÅ Setting up Rclone configuration..."
          mkdir -p ~/.config/rclone
          echo "${{ secrets.PIXELDRAIN_CONF }}" > ~/.config/rclone/rclone.conf

      - name: ‚è¨ Download Reddit Posts
        id: download-memes
        env:
          CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
          DISABLE_CACHE_CHECK: ${{ github.event.inputs.disable_cache_check }}
          SHOW_DEBUG: ${{ github.event.inputs.show_debug || true }}
          POST_LIMIT: ${{ github.event.inputs.post_limit || '5' }}
        run: |
          echo "üöÄ Starting meme download process..."
          mkdir -p cache
          python - <<EOF
          import os
          import json
          import requests
          import praw
          import yt_dlp

          client_id = os.getenv("CLIENT_ID")
          client_secret = os.getenv("CLIENT_SECRET")
          user_agent = os.getenv("USER_AGENT")
          disable_cache_check = os.getenv("DISABLE_CACHE_CHECK", "false").lower() == "true"
          show_debug = os.getenv("SHOW_DEBUG", "false").lower() == "true"
          post_limit = int(os.getenv("POST_LIMIT", "5"))
          MEME_LIMIT = 250

          def debug_log(msg, level="INFO"):
              RESET = "\033[0m"
              GREEN = "\033[92m"
              YELLOW = "\033[93m"
              RED = "\033[91m"
              UNDERLINE = "\033[4m"
              WHITE = "\033[97m"
              
              if level == "INFO":
                  color = GREEN
                  prefix = f"{color}INFO{RESET}"
              elif level == "WARNING":
                  color = YELLOW
                  prefix = f"{color}WARNING{RESET}"
              elif level == "ERROR":
                  color = RED
                  prefix = f"{color}ERROR{RESET}"
              else:
                  color = WHITE
                  prefix = f"{color}{level}{RESET}"
              
              if "<underline>" in msg and "</underline>" in msg:
                  msg = msg.replace("<underline>", UNDERLINE).replace("</underline>", RESET)
              
              print(f"{prefix} - {msg}")

          reddit = praw.Reddit(
              client_id=client_id,
              client_secret=client_secret,
              user_agent=user_agent,
          )
          debug_log("‚úÖ Reddit instance created.")

          cache_file = "cache/meme_ids.json"
          try:
              with open(cache_file, "r") as f:
                  cached_ids = set(json.load(f))
              debug_log("üìÇ Loaded cached meme IDs.")
          except:
              cached_ids = set()
              debug_log("‚ö†Ô∏è No cache found. Starting fresh.")

          valid_image_exts = ['.png','.jpg','.jpeg','.webp','.gif']
          subreddits = ["Memes", "ProgrammerHumor", "DankMemes", "DirtyMemes", "RareInsults", "Funny", "Science", "TodayILearned", "MemeVideos", "MeIRL", "Gifs", "Aww", "Videos", "AskReddit", "HolUp", "WTF", "Hmmm", "CoolGuides", "Unexpected", "SweatyPalms", "SpreadSmile", "Pranks"]
          total_memes = 0
          new_ids = []
          memes_metadata = []

          for subreddit in subreddits:
              if total_memes >= MEME_LIMIT:
                  debug_log("üìä Meme limit reached. Stopping download.")
                  break
              debug_log(f"üîç Scraping r/{subreddit} for new posts...", level="DEBUG")
              for post in reddit.subreddit(subreddit).new(limit=post_limit):
                  if not disable_cache_check and post.id in cached_ids:
                      debug_log(f"‚è≠Ô∏è Skipping cached post: {post.id}", level="INFO")
                      continue
                  try:
                      ext = os.path.splitext(post.url.lower())[1]
                      if ext in valid_image_exts:
                          debug_log(f"üì∑ Found image post: <underline>{post.id}</underline>", level="INFO")
                          r = requests.get(post.url, timeout=10)
                          r.raise_for_status()
                          out_fname = os.path.join("cache", f"{post.id}{ext}")
                          with open(out_fname, "wb") as f:
                              f.write(r.content)
                          memes_metadata.append({
                              "id": post.id,
                              "title": post.title,
                              "author": str(post.author) if post.author else "Unknown",
                              "subreddit": str(post.subreddit),
                              "upvotes": post.ups,
                              "filename": os.path.basename(out_fname),
                              "type": "image"
                          })
                          new_ids.append(post.id)
                          total_memes += 1
                          debug_log(f"‚úÖ Downloaded image: {out_fname}", level="INFO")
                      else:
                          debug_log(f"üé• Found potential video post: {post.id}", level="INFO")
                          outtmpl = f"cache/{post.id}.%(ext)s"
                          ydl_opts = {
                            'outtmpl': outtmpl,
                            'quiet': True,
                            'no_warnings': True,               # Suppresses warnings
                            'ignoreerrors': True,              # Continues on download errors
                            'format': 'best',
                            'merge_output_format': 'mp4',      # Merges into mp4 format
                            'no-progress': True,               # Disables the progress bar
                            'loglevel': 'error'                # Sets log level to show only errors
                          }
                          with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                              ydl.download([post.url])
                          downloaded_file = None
                          for file in os.listdir("cache"):
                              if file.startswith(post.id):
                                  downloaded_file = os.path.join("cache", file)
                                  break
                          if downloaded_file:
                              debug_log(f"üìπ Downloaded video: {downloaded_file}", level="INFO")
                              memes_metadata.append({
                                  "id": post.id,
                                  "title": post.title,
                                  "author": str(post.author) if post.author else "Unknown",
                                  "subreddit": str(post.subreddit),
                                  "upvotes": post.ups,
                                  "filename": os.path.basename(downloaded_file),
                                  "type": "video"
                              })
                              new_ids.append(post.id)
                              total_memes += 1
                              debug_log(f"‚úÖ Processed video post: {post.id}", level="INFO")
                          else:
                              debug_log(f"‚ö†Ô∏è Post {post.id}: Could not save as image or video - skipping.", level="WARNING")
                  except Exception as e:
                      debug_log(f"‚ö†Ô∏è Error processing post {post.id}: {e}", level="ERROR")

          if new_ids:
              cached_ids.update(new_ids)
              with open(cache_file, "w") as f:
                  json.dump(sorted(cached_ids), f)
              debug_log(f"üéâ Downloaded {len(new_ids)} new items.", level="INFO")
          else:
              debug_log("‚ú® No New Items Found!", level="INFO")
          with open("cache/memes_metadata.json", "w") as f:
              json.dump(memes_metadata, f, indent=2)
          debug_log("üìÑ Memes metadata collected.", level="INFO")
          EOF

      - name: üóÇÔ∏è Collect Metadata
        id: collect-metadata
        run: |
          echo "üìä Collecting metadata..."
          # Limit to first 250 memes to prevent matrix overflow
          limited_metadata=$(jq '.[:250]' cache/memes_metadata.json)
          metadata=$(echo "$limited_metadata" | jq -c .)
          echo "metadata=$metadata" >> $GITHUB_OUTPUT
          echo "‚úÖ Metadata collection complete."

      - name: üåê Upload Memes with Rclone
        run: |
          echo "üîÑ Uploading memes to Pixeldrain with Rclone..."
          rclone copy cache Pixeldrain:"üíØ Memes"
          echo "‚úÖ Upload complete."

      - name: üîß Compute Hash of Updated meme_ids.json
        id: compute-hash
        run: |
          echo "üîç Computing hash of meme_ids.json..."
          if [ -f cache/meme_ids.json ]; then
            FILE_HASH=$(sha256sum cache/meme_ids.json | awk '{print $1}')
            echo "üîë Computed hash: $FILE_HASH"
          else
            FILE_HASH="empty-cache"
            echo "‚ö†Ô∏è meme_ids.json not found. Using default hash."
          fi
          echo "hash=$FILE_HASH" >> $GITHUB_ENV

      - name: üíæ Update Meme IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-${{ env.hash }}

      - name: üìú List All Files
        run: |
          echo "üìÇ Listing all files in cache..."
          tree
          echo "‚úÖ File listing complete."

      - name: üì§ Upload Cache as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: cache
          path: cache/

  send-to-telegram:
    needs: download-memes
    name: üì§ Send All Memes to Telegram
    runs-on: ubuntu-latest
    steps:
      - name: üì• Check Out Repository
        uses: actions/checkout@v4

      - name: üì§ Download Cache Artifact
        uses: actions/download-artifact@v4
        with:
          name: cache
          path: cache/

      - name: üì¶ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: üß∞ Install Python Dependencies
        run: |
          echo "üì¶ Installing python-telegram-bot..."
          pip install python-telegram-bot
          echo "‚úÖ Python dependencies installed."

      - name: üì§ Send Memes in One Script
        run: |
          echo "üì§ Starting to send memes to Telegram..."
          python - <<'EOF'
          import os
          import re
          import json
          import asyncio
          from telegram import Bot

          BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
          CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
          METADATA_FILE = 'cache/memes_metadata.json'

          def escape_markdown(text):
              """Escapes Markdown-sensitive characters."""
              escape_chars = r"_*[]()~`>#+-=|{}.!"
              return re.sub(f'([{"".join(re.escape(c) for c in escape_chars)}])', r'\\\1', text)

          def build_caption(meme):
              if meme['type'] == 'image':
                  return f"""üéâ **New Meme Alert!**
          üìú *Title:* {escape_markdown(meme.get('title', 'No Title'))}
          üñãÔ∏è *Author:* {escape_markdown(meme.get('author', 'Unknown'))}
          üëç *Upvotes:* {meme.get('upvotes', '0')}
          üè∑Ô∏è *Subreddit:* r/{escape_markdown(meme.get('subreddit', '?'))}"""
              else:
                  return f"""üéâ **New Video Meme Alert!**
          üìú *Title:* {escape_markdown(meme.get('title', 'No Title'))}
          üñãÔ∏è *Author:* {escape_markdown(meme.get('author', 'Unknown'))}
          üëç *Upvotes:* {meme.get('upvotes', '0')}
          üè∑Ô∏è *Subreddit:* r/{escape_markdown(meme.get('subreddit', '?'))}"""

          async def send_meme_async(bot, meme):
              file_path = os.path.join('cache', meme['filename'])
              caption = build_caption(meme)
              if meme['type'] == 'image':
                  with open(file_path, 'rb') as img:
                      await bot.send_photo(
                          chat_id=CHAT_ID,
                          photo=img,
                          caption=caption,
                          parse_mode='Markdown'
                      )
                      print(f"üì∑ Sent image meme ID: {meme['id']}")
              elif meme['type'] == 'video':
                  with open(file_path, 'rb') as vid:
                      await bot.send_video(
                          chat_id=CHAT_ID,
                          video=vid,
                          caption=caption,
                          parse_mode='Markdown',
                          supports_streaming=True
                      )
                      print(f"üé• Sent video meme ID: {meme['id']}")

          async def main():
              bot = Bot(token=BOT_TOKEN)
              if not os.path.exists(METADATA_FILE):
                  print("‚ö†Ô∏è No metadata file found. Nothing to send.")
                  return

              with open(METADATA_FILE, 'r') as f:
                  memes = json.load(f)
              print(f"üìö Loaded {len(memes)} memes from metadata.")

              for meme in memes:
                  try:
                      await send_meme_async(bot, meme)
                  except Exception as e:
                      print(f"‚ùå Failed to send meme ID {meme['id']}: {e}")

              print("‚úÖ All memes have been processed.")

          if __name__ == '__main__':
              try:
                  asyncio.run(main())
              except Exception as e:
                  print(f"‚ùå Error sending memes in batch: {e}")
          EOF
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_TO }}
        continue-on-error: true

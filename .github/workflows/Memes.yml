name: Download Memes From Reddit

on:
  schedule:
    - cron: '0 */2 * * *' # Runs every hour
  workflow_dispatch: # Allows manual triggering of the workflow

jobs:
  download-memes:
    name: ğŸŒ Download Memes
    runs-on: ubuntu-latest
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ“¦ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: ğŸ§° Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: ğŸ”„ Restore Cache
        id: cache-restore
        uses: actions/cache@v4
        with:
          path: cache/memes.json
          key: reddit-memes-cache

      - name: â¬ Download Latest Memes
        id: download-memes
        run: |
          mkdir -p cache
          touch cache/memes.json
          python - <<EOF
          import os
          import json
          import requests

          # Reddit API URL
          url = "https://www.reddit.com/r/memes/new.json"
          headers = {"User-Agent": "GitHubActionsBot/0.1"}

          # Load cached URLs
          cache_file = "cache/memes.json"
          try:
              with open(cache_file, "r") as f:
                  cached_urls = set(json.load(f))
          except (FileNotFoundError, json.JSONDecodeError):
              cached_urls = set()

          # Fetch data from Reddit
          response = requests.get(url, headers=headers)
          response.raise_for_status()
          data = response.json()

          # Extract posts and download new images
          new_urls = []
          for post in data["data"]["children"]:
              post_data = post["data"]
              title = post_data["title"]
              author = post_data["author"]
              upvotes = post_data["ups"]
              downs = post_data["downs"]
              image_url = post_data.get("url_overridden_by_dest", "")

              if image_url.endswith((".png", ".jpg", ".jpeg")) and image_url not in cached_urls:
                  print(f"ğŸ‰ New Meme Found:\n  Title: {title}\n  Author: u/{author}\n  Upvotes: {upvotes}\n  Downs: {downs}\n  URL: {image_url}\n")
                  
                  # Save image
                  image_response = requests.get(image_url)
                  image_response.raise_for_status()
                  filename = f"cache/{os.path.basename(image_url)}"
                  with open(filename, "wb") as img_file:
                      img_file.write(image_response.content)

                  new_urls.append(image_url)

          # Update cache
          cached_urls.update(new_urls)
          with open(cache_file, "w") as f:
              json.dump(list(cached_urls), f)

          if not new_urls:
              print("âœ¨ No New Memes Found!")
          EOF

      - name: ğŸ› ï¸ Save Artifacts
        if: steps.download-memes.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: memes
          path: cache/*.png,cache/*.jpg,cache/*.jpeg

      - name: ğŸ§¹ Update Cache
        if: steps.download-memes.outcome == 'success'
        uses: actions/cache@v4
        with:
          path: cache/memes.json
          key: reddit-memes-cache

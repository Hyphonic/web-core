name: Download Posts

on:
  schedule:
    - cron: '*/15 * * * *'
  workflow_dispatch:
    inputs:
      disable_cache_check:
        description: 'Disable cache check (true/false)'
        required: false
        default: false
        type: boolean
      show_debug:
        description: 'Enable debug messages (true/false)'
        required: false
        default: false
        type: boolean
      post_limit:
        description: 'Number of posts to fetch from each subreddit'
        required: false
        default: '5'
        type: string

jobs:
  download-memes:
    name: üåê Download Memes
    runs-on: ubuntu-latest
    outputs:
      metadata: ${{ steps.collect-metadata.outputs.metadata }}
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üì¶ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: üßπ Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: meme-ids-cache-
          mode: exact
        continue-on-error: true

      - name: üöÄ Restore Meme IDs Cache
        id: restore-cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-
          restore-keys: |
            meme-ids-cache-

      - name: üß∞ Install Dependencies
        run: |
          echo "üü¢ Upgrading pip..."
          python -m pip install --upgrade pip
          echo "üü¢ Installing PRAW, Requests, and yt_dlp..."
          pip install praw requests yt-dlp

      - name: üîß Install Rclone
        run: |
          echo "üü¢ Installing Rclone..."
          curl https://rclone.org/install.sh | sudo bash

      - name: üìÇ Set Up Rclone Config
        run: |
          echo "üü¢ Setting up Rclone configuration..."
          mkdir -p ~/.config/rclone
          echo "${{ secrets.PIXELDRAIN_CONF }}" > ~/.config/rclone/rclone.conf

      - name: ‚è¨ Download Reddit Posts
        id: download-memes
        env:
          CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
          DISABLE_CACHE_CHECK: ${{ github.event.inputs.disable_cache_check }}
          SHOW_DEBUG: ${{ github.event.inputs.show_debug || true }}
          POST_LIMIT: ${{ github.event.inputs.post_limit || '5' }}
        run: |
          echo "üü¢ Starting Meme download process..."
          mkdir -p cache
          python - <<EOF
          import os
          import json
          import requests
          import praw
          import subprocess
          import sys
          from requests.adapters import HTTPAdapter
          from requests.packages.urllib3.util.retry import Retry
          
          client_id = os.getenv("CLIENT_ID")
          client_secret = os.getenv("CLIENT_SECRET")
          user_agent = os.getenv("USER_AGENT")
          disable_cache_check = os.getenv("DISABLE_CACHE_CHECK", "false").lower() == "true"
          show_debug = os.getenv("SHOW_DEBUG", "false").lower() == "true"
          post_limit = int(os.getenv("POST_LIMIT", "5"))
          MEME_LIMIT = 250
          
          def debug_log(msg):
              if show_debug:
                  print(msg)
          
          reddit = praw.Reddit(
              client_id=client_id,
              client_secret=client_secret,
              user_agent=user_agent,
          )
          debug_log("üü¢ Reddit instance created.")
          
          cache_file = "cache/meme_ids.json"
          try:
              with open(cache_file, "r") as f:
                  cached_ids = set(json.load(f))
              debug_log("üü¢ Loaded cached Meme IDs.")
          except:
              cached_ids = set()
              debug_log("üî¥ No cache found. Starting fresh.")
          
          valid_image_exts = ['.png','.jpg','.jpeg','.webp','.gif']
          subreddits = ["Memes", "ProgrammerHumor", "DankMemes", "DirtyMemes", "RareInsults", "Funny", "Science", "TodayILearned", "MemeVideos", "MeIRL", "Gifs", "Aww", "Videos", "AskReddit", "HolUp", "WTF", "Hmmm", "CoolGuides", "Unexpected", "SweatyPalms", "SpreadSmile", "Pranks"]
          total_memes = 0
          new_ids = []
          memes_metadata = []
          video_posts = []
          
          session = requests.Session()
          retry = Retry(total=3, backoff_factor=1, status_forcelist=[502, 503, 504])
          adapter = HTTPAdapter(max_retries=retry)
          session.mount('http://', adapter)
          session.mount('https://', adapter)
          
          for subreddit in subreddits:
              if total_memes >= MEME_LIMIT:
                  debug_log("üî¥ Meme limit reached. Stopping download.")
                  break
              debug_log(f"üü¢ Scraping r/{subreddit} for new posts...")
              for post in reddit.subreddit(subreddit).new(limit=post_limit):
                  if not disable_cache_check and post.id in cached_ids:
                      debug_log(f"  üîµ Skipping cached post: {post.id}")
                      continue
                  try:
                      ext = os.path.splitext(post.url.lower())[1]
                      if ext in valid_image_exts:
                          debug_log(f"  üü° Found image post: {post.id}")
                          r = session.get(post.url, timeout=10)
                          r.raise_for_status()
                          out_fname = os.path.join("cache", f"{post.id}{ext}")
                          with open(out_fname, "wb") as f:
                              f.write(r.content)
                          memes_metadata.append({
                              "id": post.id,
                              "title": post.title,
                              "author": str(post.author) if post.author else "Unknown",
                              "subreddit": str(post.subreddit),
                              "upvotes": post.ups,
                              "filename": os.path.basename(out_fname),
                              "type": "image"
                          })
                          new_ids.append(post.id)
                          total_memes += 1
                          debug_log(f"    üü° Downloaded image: {out_fname}")
                      else:
                          debug_log(f"  üü† Found potential video post: {post.id}")
                          video_posts.append(post)
                  except Exception as e:
                      debug_log(f"  üî¥ Error processing post {post.id}: {e}")
          
          for post in video_posts:
              try:
                  outtmpl = f"cache/{post.id}.%(ext)s"
                  with open(os.devnull, 'w') as devnull:
                      result = subprocess.run([
                          sys.executable, '-m', 'yt_dlp', '--quiet', '--no-warnings', '--ignore-errors',
                          '--format', 'best', '--merge-output-format', 'mp4', '--output', outtmpl, post.url
                      ], stdout=devnull, stderr=devnull)
          
                  downloaded_file = None
                  for file in os.listdir("cache"):
                      if file.startswith(post.id):
                          downloaded_file = os.path.join("cache", file)
                          break
                  if downloaded_file:
                      debug_log(f"  üü° Downloaded video: {downloaded_file}")
                      memes_metadata.append({
                          "id": post.id,
                          "title": post.title,
                          "author": str(post.author) if post.author else "Unknown",
                          "subreddit": str(post.subreddit),
                          "upvotes": post.ups,
                          "filename": os.path.basename(downloaded_file),
                          "type": "video"
                      })
                      new_ids.append(post.id)
                      total_memes += 1
              except Exception as e:
                  debug_log(f"  üî¥ Error processing video post {post.id}: {e}")

          if new_ids:
              cached_ids.update(new_ids)
              with open(cache_file, "w") as f:
                  json.dump(sorted(cached_ids), f)
              debug_log(f"üü¢ Downloaded {len(new_ids)} new items.")
          else:
              debug_log("üü† No New Items Found!")
          
          with open("cache/memes_metadata.json", "w") as f:
              json.dump(memes_metadata, f, indent=2)
          debug_log("üü¢ Memes metadata collected.")
          EOF

      - name: üóÇÔ∏è Collect Metadata
        id: collect-metadata
        run: |
          echo "üü¢ Collecting metadata..."
          # Limit to first 250 memes to prevent matrix overflow
          limited_metadata=$(jq '.[:250]' cache/memes_metadata.json)
          metadata=$(echo "$limited_metadata" | jq -c .)
          echo "metadata=$metadata" >> $GITHUB_OUTPUT
          echo "üü¢ Metadata collection complete."

      - name: üåê Upload Memes with Rclone
        run: |
          echo "üü¢ Uploading memes to Pixeldrain with Rclone..."
          rclone copy cache Pixeldrain:"üíØ Memes"
          echo "üü¢ Upload complete."

      - name: üîß Compute Hash of Updated meme_ids.json
        id: compute-hash
        run: |
          echo "üü¢ Computing hash of meme_ids.json..."
          if [ -f cache/meme_ids.json ]; then
            FILE_HASH=$(sha256sum cache/meme_ids.json | awk '{print $1}')
            echo "üü¢ Computed hash: $FILE_HASH"
          else
            FILE_HASH="empty-cache"
            echo "üî¥ meme_ids.json not found. Using default hash."
          fi
          echo "hash=$FILE_HASH" >> $GITHUB_ENV

      - name: üíæ Update Meme IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-${{ env.hash }}

      - name: üìú List All Files
        run: |
          echo "üü¢ Listing all files in cache..."
          tree
          echo "üü¢ File listing complete."

      - name: üì§ Upload Cache as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: meme-cache
          path: cache/

  download-coomer:
    name: üåê Download Coomer
    runs-on: ubuntu-latest
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üì¶ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      
      - name: üßπ Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: coomer-ids-cache-
          mode: exact
        continue-on-error: true

      - name: üöÄ Restore Coomer IDs Cache
        id: restore-coomer-cache
        uses: actions/cache@v4
        with:
          path: cache/coomer_ids.json
          key: coomer-ids-cache-
          restore-keys: |
            coomer-ids-cache-

      - name: üß∞ Install Dependencies
        run: |
          echo "üü¢ Installing dependencies..."
          python -m pip install --upgrade pip
          pip install requests

      - name: üîß Install Rclone
        run: |
          echo "üü¢ Installing Rclone..."
          curl https://rclone.org/install.sh | sudo bash

      - name: üìÇ Set Up Rclone Config
        run: |
          echo "üü¢ Setting up Rclone configuration..."
          mkdir -p ~/.config/rclone
          echo "${{ secrets.PIXELDRAIN_CONF }}" > ~/.config/rclone/rclone.conf

      - name: ‚è¨ Download Coomer Posts (Parallel)
        run: |
          echo "üü¢ Starting Coomer download process with parallel requests..."
          mkdir -p cache
          python - <<EOF
          import os
          import json
          import requests
          import concurrent.futures
          from requests.adapters import HTTPAdapter
          from requests.packages.urllib3.util.retry import Retry

          disable_cache_check = False
          show_debug = True
          creators = ["darcyxnycole", "belledelphine", "sweetiefox_of"]  # Add more creators as needed
          o_limit = 50  # Number of posts to fetch per creator
          cache_file = "cache/coomer_ids.json"
          TIMEOUT_SECONDS = 300  # 5 minutes
          MAX_WORKERS = 8        # Adjust as needed

          def debug_log(msg):
              if show_debug:
                  print(msg)

          def download_file(download_url, out_fname, file_id):
              try:
                  r = session.get(download_url, timeout=TIMEOUT_SECONDS)
                  r.raise_for_status()
                  with open(out_fname, "wb") as out:
                      out.write(r.content)
                  return True
              except Exception as e:
                  debug_log(f"  üî¥ Error downloading file {file_id}: {e}")
                  return False

          try:
              with open(cache_file, "r") as f:
                  cached_ids = set(json.load(f))
              debug_log("üü¢ Loaded cached Coomer IDs.")
          except:
              cached_ids = set()
              debug_log("üî¥ No cache found. Starting fresh.")

          unique_tasks = {}
          new_ids = []

          session = requests.Session()
          retry = Retry(total=3, backoff_factor=1, status_forcelist=[502, 503, 504])
          adapter = HTTPAdapter(max_retries=retry)
          session.mount('http://', adapter)
          session.mount('https://', adapter)

          for creator in creators:
              coomer_url = f"https://coomer.su/api/v1/onlyfans/user/{creator}?o={o_limit}"
              debug_log(f"üü¢ Fetching data from {coomer_url}")
              try:
                  resp = session.get(coomer_url, timeout=TIMEOUT_SECONDS)
                  resp.raise_for_status()
                  items = resp.json()
              except Exception as e:
                  debug_log(f"üî¥ Failed to fetch data for {creator}: {e}")
                  continue

              for item in items:
                  file_id = str(item.get('id', ''))
                  if file_id in cached_ids and not disable_cache_check:
                      debug_log(f"  üîµ Skipping cached file: {file_id}")
                      continue

                  paths = set()
                  if 'file' in item and 'path' in item['file']:
                      paths.add(item['file']['path'])
                  for att in item.get('attachments', []):
                      p = att.get('path')
                      if p:
                          paths.add(p)

                  for p in paths:
                      download_url = "https://coomer.su" + p
                      # Create directories per creator
                      creator_dir = os.path.join("cache", creator)
                      os.makedirs(creator_dir, exist_ok=True)
                      out_fname = os.path.join(creator_dir, f"{file_id}-{os.path.basename(p)}")
                      # Deduplicate by url/out_fname across all creators
                      unique_tasks[(download_url, out_fname)] = file_id

          tasks = [(k[0], k[1], v) for k, v in unique_tasks.items()]
          total_tasks = len(tasks)

          debug_log(f"üü¢ Starting parallel downloads for {total_tasks} unique files.")
          completed = 0
          with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
              future_map = {
                  executor.submit(download_file, url, fname, fid): (url, fname, fid)
                  for (url, fname, fid) in tasks
              }
              for future in concurrent.futures.as_completed(future_map):
                  url, fname, fid = future_map[future]
                  success = future.result()
                  completed += 1
                  percent = int((completed / total_tasks) * 100)
                  if success:
                      new_ids.append(fid)
                      debug_log(f"  üü° ({percent}%) Downloaded {fid} -> {fname}")
                  else:
                      debug_log(f"  üî¥ ({percent}%) Failed {fid}")

          if new_ids:
              cached_ids.update(new_ids)
              with open(cache_file, "w") as f:
                  json.dump(sorted(cached_ids), f)
              debug_log(f"üü¢ Downloaded {len(new_ids)} new Coomer files.")
          else:
              debug_log("üü† No New Items Found!")
          EOF

      - name: üåê Upload Coomer Posts with Rclone
        run: |
          echo "üü¢ Uploading coomer posts to Pixeldrain with Rclone..."
          rclone copy cache Pixeldrain:"üåÄ Onlyfans"
          echo "üü¢ Upload complete."

      - name: üîß Compute Hash of Updated coomer_ids.json
        id: compute-hash-coomer
        run: |
          echo "üü¢ Computing hash of coomer_ids.json..."
          if [ -f cache/coomer_ids.json ]; then
            COOMER_HASH=$(sha256sum cache/coomer_ids.json | awk '{print $1}')
            echo "üü¢ Computed hash: $COOMER_HASH"
          else
            COOMER_HASH="empty-cache"
            echo "üî¥ coomer_ids.json not found. Using default hash."
          fi
          echo "coomer_hash=$COOMER_HASH" >> $GITHUB_ENV

      - name: üíæ Update Coomer IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/coomer_ids.json
          key: coomer-ids-cache-${{ env.coomer_hash }}

      - name: üìú List All Coomer Files
        run: |
          echo "üü¢ Listing Coomer files in cache..."
          tree cache
          echo "üü¢ File listing complete."

      - name: üì§ Upload Cache as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: coomer-cache
          path: cache/

  send-to-telegram:
    needs: download-memes
    name: üì§ Send All Memes to Telegram
    runs-on: ubuntu-latest
    steps:
      - name: üì• Check Out Repository
        uses: actions/checkout@v4

      - name: üì§ Download Cache Artifact
        uses: actions/download-artifact@v4
        with:
          name: meme-cache
          path: cache/

      - name: üì¶ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: üß∞ Install Python Dependencies
        run: |
          echo "üü¢ Installing python-telegram-bot..."
          pip install python-telegram-bot
          echo "üü¢ Python dependencies installed."

      - name: üì§ Send Memes in One Script
        run: |
          echo "üì§ Starting to send memes to Telegram..."
          python - <<'EOF'
          import os
          import re
          import json
          import asyncio
          from telegram import Bot
          
          BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
          CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
          METADATA_FILE = 'cache/memes_metadata.json'
          
          def escape_markdown(text):
          	"""Escapes Markdown-sensitive characters."""
          	escape_chars = r"_*[]()~`>#+-=|{}.!"
          	return re.sub(f'([{"".join(re.escape(c) for c in escape_chars)}])', r'\\\1', text)
          
          def build_caption(meme):
          	if meme['type'] == 'image':
          		return f"""üéâ **New Meme Alert!**
          üìú *Title:* {escape_markdown(meme.get('title', 'No Title'))}
          üñãÔ∏è *Author:* {escape_markdown(meme.get('author', 'Unknown'))}
          üëç *Upvotes:* {meme.get('upvotes', '0')}
          üè∑Ô∏è *Subreddit:* r/{escape_markdown(meme.get('subreddit', '?'))}"""
          	else:
          		return f"""üéâ **New Video Meme Alert!**
          üìú *Title:* {escape_markdown(meme.get('title', 'No Title'))}
          üñãÔ∏è *Author:* {escape_markdown(meme.get('author', 'Unknown'))}
          üëç *Upvotes:* {meme.get('upvotes', '0')}
          üè∑Ô∏è *Subreddit:* r/{escape_markdown(meme.get('subreddit', '?'))}"""
          
          async def send_meme_async(bot, meme):
          	file_path = os.path.join('cache', meme['filename'])
          	caption = build_caption(meme)
          	if meme['type'] == 'image':
          		with open(file_path, 'rb') as img:
          			await bot.send_photo(
          				chat_id=CHAT_ID,
          				photo=img,
          				caption=caption,
          				parse_mode='Markdown'
          			)
          			print(f"üü¢ Sent image Meme ID: {meme['id']}")
          	elif meme['type'] == 'video':
          		with open(file_path, 'rb') as vid:
          			await bot.send_video(
          				chat_id=CHAT_ID,
          				video=vid,
          				caption=caption,
          				parse_mode='Markdown',
          				supports_streaming=True
          			)
          			print(f"üü¢ Sent video Meme ID: {meme['id']}")
          
          async def main():
          	bot = Bot(token=BOT_TOKEN)
          	if not os.path.exists(METADATA_FILE):
          		print("üü† No metadata file found. Nothing to send.")
          		return
          
          	with open(METADATA_FILE, 'r') as f:
          		memes = json.load(f)
          	print(f"üü¢ Loaded {len(memes)} memes from metadata.")
          
          	for meme in memes:
          		try:
          			await send_meme_async(bot, meme)
          		except Exception as e:
          			print(f"üî¥ Failed to send Meme ID {meme['id']}: {e}")
          
          	print("üü¢ All memes have been processed.")
          
          if __name__ == '__main__':
          	try:
          		asyncio.run(main())
          	except Exception as e:
          		print(f"üî¥ Error sending memes in batch: {e}")
          EOF
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_TO }}
        continue-on-error: true

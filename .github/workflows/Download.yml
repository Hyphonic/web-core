name: Download Posts

on:
  schedule:
    - cron: "*/5 * * * *"
  workflow_dispatch:
    inputs:
      disable_cache_check:
        description: '[Global] Disable cache check (true/false)'
        required: false
        default: false
        type: boolean
      show_debug:
        description: '[Global] Enable debug messages (true/false)'
        required: true
        default: true
        type: boolean
      post_limit:
        description: '[Reddit] Number of posts to fetch from each subreddit'
        required: false
        default: '5'
        type: string
      target_posts_coomer:
        description: '[Coomer] Number of posts to fetch per creator'
        required: false
        default: '300'
        type: string
      max_urls_coomer:
        description: '[Coomer] Maximum total URLs to download'
        required: false
        default: '250'
        type: string
      creators:
        description: '[Coomer] Comma-separated list of creators (leave empty for default)'
        required: false
        type: string
      target_posts_kemono:
        description: '[Kemono] Number of posts to fetch per creator'
        required: false
        default: '150'
        type: string
      max_urls_kemono:
        description: '[Kemono] Maximum total URLs to download'
        required: false
        default: '100'
        type: string
      target_posts_rule34:
        description: '[Rule34] Number of posts to fetch per creator'
        required: false
        default: '300'
        type: string
      max_urls_rule34:
        description: '[Rule34] Maximum total URLs to download'
        required: false
        default: '250'
        type: string

# Add concurrency group to prevent parallel runs
#concurrency:
#  group: ${{ github.workflow }}
#  cancel-in-progress: true

jobs:
  check-workflows:
    name: ðŸ” Check Running Workflows
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - name: ðŸ” Check for running workflows
        id: check
        run: |
          # Get running workflows excluding current
          RUNNING=$(gh api \
            -H "Accept: application/vnd.github+json" \
            /repos/${{ github.repository }}/actions/runs \
            --jq '.workflow_runs[] | select(.status=="in_progress" or .status=="queued") | select(.id!=${{ github.run_id }}) | .id' \
            | wc -l)
          
          if [ "$RUNNING" -gt "0" ]; then
            echo "ðŸ”´ Found $RUNNING other running workflows - skipping"
            echo "should_run=false" >> $GITHUB_OUTPUT
          else
            echo "ðŸŸ¢ No other workflows running"
            echo "should_run=true" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ github.token }}

  download-memes:
    needs: check-workflows
    if: needs.check-workflows.outputs.should_run == 'true'
    name: ðŸŒ Download Memes
    runs-on: ubuntu-latest
    outputs:
      metadata: ${{ steps.collect-metadata.outputs.metadata }}
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ“¦ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip' # caching pip dependencies
      - run: pip install -r requirements.txt

      - name: ðŸ§¹ Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: meme-ids-cache-
          mode: exact
        continue-on-error: true

      - name: ðŸš€ Restore Meme IDs Cache
        id: restore-cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-
          restore-keys: |
            meme-ids-cache-
      - name: ðŸ”§ Install Rclone
        uses: AnimMouse/setup-rclone@v1
        with:
          rclone_config: ${{ secrets.PIXELDRAIN_CONF }}
          disable_base64: true
      - name: â¬ Download Reddit Posts
        id: download-memes
        run: |
          echo "ðŸŸ¢ Starting Meme download process..."
          python scripts/reddit_downloader.py \
            --client-id "${{ secrets.REDDIT_CLIENT_ID }}" \
            --client-secret "${{ secrets.REDDIT_CLIENT_SECRET }}" \
            --user-agent "Average Upvote v1.0 by /u/Hyphonical" \
            --post-limit "${{ inputs.post_limit || '5' }}" \
            $([ "${{ inputs.show_debug }}" == "true" ] && echo "--debug") \
            $([ "${{ inputs.disable_cache_check }}" == "true" ] && echo "--disable-cache")
      - name: ðŸ—‚ï¸ Collect Metadata
        id: collect-metadata
        run: |
          echo "ðŸŸ¢ Collecting metadata..."
          limited_metadata=$(jq '.[:250]' cache/memes_metadata.json)
          metadata=$(echo "$limited_metadata" | jq -c .)
          echo "metadata=$metadata" >> $GITHUB_OUTPUT
          echo "ðŸŸ¢ Metadata collection complete."
      - name: ðŸŒ Upload Memes with Rclone
        run: |
          echo "ðŸŸ¢ Uploading memes to Pixeldrain..."
          rclone copy cache Pixeldrain:"ðŸ’¯ Memes" --disable-http2 --multi-thread-streams 6 --transfers 8 -v
          echo "ðŸŸ¢ Upload complete."
      - name: ðŸ”§ Compute Hash of Updated meme_ids.json
        id: compute-hash
        run: |
          echo "ðŸŸ¢ Computing hash of meme_ids.json..."
          if [ -f cache/meme_ids.json]; then
            FILE_HASH=$(sha256sum cache/meme_ids.json | awk '{print $1}')
            echo "ðŸŸ¢ Computed hash: $FILE_HASH"
          else
            FILE_HASH="empty-cache"
            echo "ðŸ”´ meme_ids.json not found. Using default hash."
          fi
          echo "hash=$FILE_HASH" >> $GITHUB_ENV
      - name: ðŸ’¾ Update Meme IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/meme_ids.json
          key: meme-ids-cache-${{ env.hash }}

      - name: ðŸ“œ List All Files
        run: |
          echo "ðŸŸ¢ Listing all files in cache..."
          tree
          echo "ðŸŸ¢ File listing complete."
      - name: ðŸ“¤ Upload Cache as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: meme-cache
          path: cache/

  download-coomer:
    needs: check-workflows
    if: needs.check-workflows.outputs.should_run == 'true'
    name: ðŸŒ Download Coomer
    runs-on: ubuntu-latest
    steps:
      - name: ðŸ§¹ Free Up Space
        uses: jlumbroso/free-disk-space@main
        with:
          # this might remove tools that are actually needed,
          # if set to "true" but frees about 6 GB
          tool-cache: true
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ“¦ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip' # caching pip dependencies
      - run: pip install -r requirements.txt
      
      - name: ðŸ§¹ Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: coomer-ids-cache-
          mode: exact
        continue-on-error: true

      - name: ðŸš€ Restore Coomer IDs Cache
        id: restore-coomer-cache
        uses: actions/cache@v4
        with:
          path: cache/coomer_ids.json
          key: coomer-ids-cache-
          restore-keys: |
            coomer-ids-cache-
      - name: ðŸ”§ Install Rclone
        uses: AnimMouse/setup-rclone@v1
        with:
          rclone_config: ${{ secrets.PIXELDRAIN_CONF }}
          disable_base64: true
      - name: â¬ Download Coomer Posts (Parallel)
        env:
          OF_CREATORS: ${{ secrets.CREATORS_LIST_OF }}
          FANSLY_CREATORS: ${{ secrets.CREATORS_LIST_FANSLY }}
        run: |
          python scripts/coomer_downloader.py \
            $([ "${{ inputs.show_debug }}" == "true" ] && echo "--debug") \
            $([ "${{ inputs.disable_cache_check }}" == "true" ] && echo "--disable-cache") \
            --target-posts "${{ inputs.target_posts_coomer || '300' }}" \
            --max-urls "${{ inputs.max_urls_coomer || '250' }}" \
            --of-creators "$OF_CREATORS" \
            --fansly-creators "$FANSLY_CREATORS"
      - name: ðŸŒ Upload Coomer Posts with Rclone
        run: |
          echo "ðŸŸ¢ Uploading coomer posts to Pixeldrain with Rclone..."
          rclone copy cache Pixeldrain:"ðŸŒ€ Onlyfans" --disable-http2 --multi-thread-streams 6 --transfers 8 -v
          echo "ðŸŸ¢ Upload complete."
      - name: ðŸ”§ Compute Hash of Updated coomer_ids.json
        id: compute-hash-coomer
        run: |
          echo "ðŸŸ¢ Computing hash of coomer_ids.json..."
          if [ -f cache/coomer_ids.json ]; then 
            COOMER_HASH=$(sha256sum cache/coomer_ids.json | awk '{print $1}')
            echo "ðŸŸ¢ Computed hash: $COOMER_HASH"
          else
            COOMER_HASH="empty-cache"
            echo "ðŸ”´ coomer_ids.json not found. Using default hash."
          fi
          echo "coomer_hash=$COOMER_HASH" >> $GITHUB_ENV
      - name: ðŸ’¾ Update Coomer IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/coomer_ids.json
          key: coomer-ids-cache-${{ env.coomer_hash }}

      - name: ðŸ“œ List All Coomer Files
        run: |
          echo "ðŸŸ¢ Listing Coomer files in cache..."
          tree cache
          echo "ðŸŸ¢ File listing complete."

  download-kemono:
    needs: check-workflows
    if: needs.check-workflows.outputs.should_run == 'true'
    name: ðŸŒ Download Kemono
    runs-on: ubuntu-latest
    steps:
      - name: ðŸ§¹ Free Up Space
        uses: jlumbroso/free-disk-space@main
        with:
          # this might remove tools that are actually needed,
          # if set to "true" but frees about 6 GB
          tool-cache: true
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ“¦ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip' # caching pip dependencies
      - run: pip install -r requirements.txt
      
      - name: ðŸ§¹ Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: kemono-ids-cache-
          mode: exact
        continue-on-error: true

      - name: ðŸš€ Restore Kemono IDs Cache
        id: restore-kemono-cache
        uses: actions/cache@v4
        with:
          path: cache/kemono_ids.json
          key: kemono-ids-cache-
          restore-keys: |
            kemono-ids-cache-
      - name: ðŸ”§ Install Rclone
        uses: AnimMouse/setup-rclone@v1
        with:
          rclone_config: ${{ secrets.PIXELDRAIN_CONF }}
          disable_base64: true
      - name: â¬ Download Kemono Posts
        env:
          KEMONO_CREATORS: ${{ secrets.CREATORS_LIST_KEMONO }}
        run: |
          python scripts/kemono_downloader.py \
            --creators "$KEMONO_CREATORS" \
            $([ "${{ inputs.show_debug }}" == "true" ] && echo "--debug") \
            $([ "${{ inputs.disable_cache_check }}" == "true" ] && echo "--disable-cache") \
            --target-posts "${{ inputs.target_posts_kemono || '150' }}" \
            --max-urls "${{ inputs.max_urls_kemono || '100' }}"
      - name: ðŸŒ Upload Kemono Posts with Rclone
        run: |
          echo "ðŸŸ¢ Uploading Kemono posts to Pixeldrain..."
          rclone copy cache Pixeldrain:"ðŸ…¿ï¸ Patreon" --disable-http2 --multi-thread-streams 6 --transfers 16 -v
          echo "ðŸŸ¢ Upload complete."
      - name: ðŸ”§ Compute Hash of Updated kemono_ids.json
        id: compute-hash-kemono
        run: |
          echo "ðŸŸ¢ Computing hash of kemono_ids.json..."
          if [ -f cache/kemono_ids.json ]; then 
            KEMONO_HASH=$(sha256sum cache/kemono_ids.json | awk '{print $1}')
            echo "ðŸŸ¢ Computed hash: $KEMONO_HASH"
          else
            KEMONO_HASH="empty-cache"
            echo "ðŸ”´ kemono_ids.json not found. Using default hash."
          fi
          echo "kemono_hash=$KEMONO_HASH" >> $GITHUB_ENV
      - name: ðŸ’¾ Update Kemono IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/kemono_ids.json
          key: kemono-ids-cache-${{ env.kemono_hash }}

      - name: ðŸ“œ List All Kemono Files
        run: |
          echo "ðŸŸ¢ Listing Kemono files in cache..."
          tree cache
          echo "ðŸŸ¢ File listing complete."

  download-rule34:
    needs: check-workflows
    if: needs.check-workflows.outputs.should_run == 'true'
    name: ðŸŒ Download Rule34
    runs-on: ubuntu-latest
    steps:
      - name: ðŸ§¹ Free Up Space
        uses: jlumbroso/free-disk-space@main
        with:
          tool-cache: true

      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ“¦ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip'
      - run: pip install -r requirements.txt
      
      - name: ðŸ§¹ Remove Cache
        uses: DareFox/delete-cache-by-key@v1
        with:
          key: rule34-ids-cache-
          mode: exact
        continue-on-error: true

      - name: ðŸš€ Restore Rule34 IDs Cache
        id: restore-rule34-cache
        uses: actions/cache@v4
        with:
          path: cache/rule34_ids.json
          key: rule34-ids-cache-
          restore-keys: |
            rule34-ids-cache-

      - name: ðŸ”§ Install Rclone
        uses: AnimMouse/setup-rclone@v1
        with:
          rclone_config: ${{ secrets.PIXELDRAIN_CONF }}
          disable_base64: true

      - name: â¬ Download Rule34 Posts
        env:
          CREATORS: ${{ secrets.CREATORS_LIST_RULE34 }}
        run: |
          python scripts/rule34_downloader.py \
            $([ "${{ inputs.show_debug }}" == "true" ] && echo "--debug") \
            $([ "${{ inputs.disable_cache_check }}" == "true" ] && echo "--disable-cache") \
            --target-posts "${{ inputs.target_posts_rule34 || '300' }}" \
            --max-urls "${{ inputs.max_urls_rule34 || '250' }}" \
            --creators "$CREATORS"

      - name: ðŸŒ Upload Rule34 Posts with Rclone
        run: |
          echo "ðŸŸ¢ Uploading rule34 posts to Pixeldrain..."
          rclone copy cache Pixeldrain:"ðŸŽ¨ Rule34" --disable-http2 --multi-thread-streams 6 --transfers 8 -v
          echo "ðŸŸ¢ Upload complete."

      - name: ðŸ”§ Compute Hash of Updated rule34_ids.json
        id: compute-hash-rule34
        run: |
          echo "ðŸŸ¢ Computing hash of rule34_ids.json..."
          if [ -f cache/rule34_ids.json ]; then 
            RULE34_HASH=$(sha256sum cache/rule34_ids.json | awk '{print $1}')
            echo "ðŸŸ¢ Computed hash: $RULE34_HASH"
          else
            RULE34_HASH="empty-cache"
            echo "ðŸ”´ rule34_ids.json not found. Using default hash."
          fi
          echo "rule34_hash=$RULE34_HASH" >> $GITHUB_ENV

      - name: ðŸ’¾ Update Rule34 IDs Cache
        uses: actions/cache@v4
        with:
          path: cache/rule34_ids.json
          key: rule34-ids-cache-${{ env.rule34_hash }}

      - name: ðŸ“œ List All Rule34 Files
        run: |
          echo "ðŸŸ¢ Listing Rule34 files in cache..."
          tree cache
          echo "ðŸŸ¢ File listing complete."

  send-to-telegram:
    needs: download-memes
    name: ðŸ“¤ Send All Memes to Telegram
    runs-on: ubuntu-latest
    steps:
      - name: ðŸ“¥ Check Out Repository
        uses: actions/checkout@v4
      - name: ðŸ“¤ Download Cache Artifact
        uses: actions/download-artifact@v4
        with:
          name: meme-cache
          path: cache/
      - name: ðŸ“¦ Set Up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
          cache: 'pip' # caching pip dependencies
      - run: pip install -r requirements.txt
      - name: ðŸ“¤ Send Memes to Telegram
        run: |
          echo "ðŸ“¤ Starting to send memes to Telegram..."
          python scripts/telegram_sender.py \
            --token "${{ secrets.TELEGRAM_TOKEN }}" \
            --chat-id "${{ secrets.TELEGRAM_TO }}" \
            $([ "${{ inputs.show_debug }}" == "true" ] && echo "--debug")
        continue-on-error: true
